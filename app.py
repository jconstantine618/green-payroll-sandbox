# app.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
Green-Payroll Sales Trainer   (Streamlit + OpenAI + voice I/O)

â€¢  Tick â€œSpeak instead of typeâ€ â†’ press Start, talk, Stop, then Send.
â€¢  Assistant can reply with TTS if â€œRead assistant replies aloudâ€ checked.

Python â‰¥3.9   â€¢   requires the packages listed in requirements.txt
"""

from __future__ import annotations
import os, io, wave, tempfile, pathlib
import numpy as np
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI, BadRequestError
from streamlit_webrtc import webrtc_streamer, WebRtcMode

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()                              # .env in local dev
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
ELEVEN_API_KEY = os.getenv("ELEVEN_API_KEY")          # optional
TMP_DIR        = pathlib.Path(tempfile.gettempdir())
WHISPER_MODEL  = "whisper-1"
GPT_MODEL      = "gpt-4o-mini"
ASSISTANT_VOICE= "elevenlabs/Antoni"                  # or any voice

client = OpenAI(api_key=OPENAI_API_KEY)

st.set_page_config("Sales-Trainer", "ğŸ¤", layout="wide")
st.sidebar.title("âš™ï¸ Controls")

# â”€â”€ SCENARIOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SCENARIOS = {
    "Sunshine Daycare Centers (Child-care)": dict(
        persona    = "Karen Lopez (Owner / Director)",
        background = "Handles HR and scheduling. Wants mobile access.",
        company    = "Sunshine Daycare Centers",
        difficulty = "Easy",
        time       = "10 min",
        prompt     = (
            "You are a payroll SaaS sales rep calling Karen Lopez at "
            "Sunshine Daycare Centers. Ask discovery questions with empathy "
            "and keep responses short."
        ),
    ),
    # â€¦add more scenarios hereâ€¦
}

# â”€â”€ TTS UTIL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def tts_bytes(text: str) -> io.BytesIO | None:
    """Return MP3 bytes via ElevenLabs (preferred) or gTTS fallback."""
    if ELEVEN_API_KEY:
        try:
            from elevenlabs import generate, VoiceSettings, set_api_key
            set_api_key(ELEVEN_API_KEY)
            audio = generate(
                text=text,
                voice=ASSISTANT_VOICE.split("/")[-1],
                model="eleven_multilingual_v2",
                voice_settings=VoiceSettings(stability=0.35, similarity_boost=0.7),
            )
            return io.BytesIO(audio)
        except Exception as e:
            st.warning(f"ElevenLabs TTS failed ({e}); falling back to gTTSâ€¦")

    try:
        from gtts import gTTS
        buf = io.BytesIO()
        gTTS(text=text, lang="en", slow=False).write_to_fp(buf)
        buf.seek(0)
        return buf
    except Exception as e:
        st.warning(f"gTTS failed ({e}). No audio will be played.")
        return None

# â”€â”€ VOICE-IN  (WebRTC â†’ Whisper) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def record_and_transcribe() -> str | None:
    """
    1.  Creates / reuses a WebRTC recorder component.
    2.  Waits until the user *stops* recording and presses â€œSendâ€.
    3.  Saves audio, sends to Whisper, returns transcript text.
    """
    ctx = webrtc_streamer(
        key="speech",
        mode=WebRtcMode.SENDONLY,
        media_stream_constraints={"video": False, "audio": True},
        sendback_audio=False,
        # â†“ hide device selector with CSS; still functional
        translations={"select_device": ""},
    )

    #  We use Streamlit session-state flags to know recording status
    if "audio_ready" not in st.session_state:
        st.session_state.audio_ready = False

    # â€” UI helper â€”
    def _send_btn_disabled() -> bool:
        return not (ctx and not ctx.state.playing and st.session_state.audio_ready)

    st.button(
        "â–¶ï¸ Send recording",
        key="send_audio_btn",
        disabled=_send_btn_disabled(),
    )

    # 1ï¸âƒ£  Recording phase
    if ctx.state.playing:
        st.session_state.audio_ready = True     # weâ€™ll have something to fetch
        st.info("Recordingâ€¦ press **Stop** when finished, then click **Send**.")
        st.stop()

    # 2ï¸âƒ£  Waiting for user to hit â€œSend recordingâ€
    if st.session_state.get("send_audio_btn") is False:
        st.stop()

    # 3ï¸âƒ£  Collect frames only once after Send
    if ctx.audio_receiver and st.session_state.audio_ready:
        frames = ctx.audio_receiver.get_frames(timeout=1)
        if not frames:
            st.warning("No speech detected. Try again.")
            st.session_state.audio_ready = False
            return None

        samples = np.concatenate([f.to_ndarray().flatten() for f in frames])
        sr      = frames[0].sample_rate
        wav_path = TMP_DIR / "speech.wav"
        with wave.open(wav_path, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(sr)
            wf.writeframes(samples.tobytes())

        try:
            rsp = client.audio.transcriptions.create(
                model=WHISPER_MODEL,
                file=open(wav_path, "rb"),
                response_format="text",
            )
            st.session_state.audio_ready = False  # reset for next turn
            return rsp.text.strip()
        except BadRequestError:
            st.warning("Whisper could not transcribe audio. Try again.")
            st.session_state.audio_ready = False
            return None

# â”€â”€ GPT CHAT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat(user_msg: str) -> str:
    history = st.session_state.history
    msgs = history + [{"role": "user", "content": user_msg}]
    rsp = client.chat.completions.create(model=GPT_MODEL, messages=msgs)
    assistant_msg = rsp.choices[0].message.content.strip()
    history.extend([{"role": "user", "content": user_msg},
                    {"role": "assistant", "content": assistant_msg}])
    return assistant_msg

# â”€â”€ STATE INIT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "history" not in st.session_state:
    st.session_state.history = []

# â”€â”€ SIDEBAR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
scenario_name = st.sidebar.selectbox("Choose a scenario", SCENARIOS.keys())
speak_mode    = st.sidebar.checkbox("ğŸ¤ Speak instead of type", value=False)
voice_reply   = st.sidebar.checkbox("ğŸ”Š Read assistant replies aloud")

# â”€â”€ HEADER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sc = SCENARIOS[scenario_name]
st.title("ğŸ’¬ Chatbot")
st.markdown(
    f"""
**Persona**: {sc['persona']}  
**Background**: {sc['background']}  
**Company**: {sc['company']}  
**Difficulty**: {sc['difficulty']}  
**Time Available**: {sc['time']}  
"""
)

if not st.session_state.history:
    st.session_state.history.append({"role": "system", "content": sc["prompt"]})

# â”€â”€ CONVERSATION FLOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def handle_turn():
    # â‘   Get user input
    user_text: str | None
    if speak_mode:
        user_text = record_and_transcribe()
    else:
        user_text = st.chat_input("Your message to the prospect")

    if not user_text:
        return

    # â‘¡  Show user bubble
    with st.chat_message("user"):
        st.write(user_text)

    # â‘¢  Assistant response
    assistant_text = chat(user_text)
    with st.chat_message("assistant"):
        st.write(assistant_text)
        if voice_reply:
            audio = tts_bytes(assistant_text)
            if audio:
                st.audio(audio, format="audio/mp3")

handle_turn()

# â”€â”€ DEBUG (collapsed by default) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.expander("ğŸ“œ Conversation (debug)", expanded=False):
    st.json(st.session_state.history)

# â”€â”€ HIDE â€œSELECT DEVICEâ€ LABEL WITH CSS (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    """
    <style>
    label:has(> span[data-testid="stMarkdownContainer"]:contains("select device")) {
        display:none !important;
    }
    </style>""",
    unsafe_allow_html=True,
)
